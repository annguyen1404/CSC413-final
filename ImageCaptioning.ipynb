{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageCaptioning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVqlPrHqwlYiigoYjLpJHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annguyen1404/CSC413-final/blob/main/ImageCaptioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We introduce two attention-based image caption generators under a common framework \n",
        "1. “soft” deterministic attention mechanism trainable by\n",
        "standard back-propagation methods\n",
        "2. “hard” stochastic attention mechanism trainable by maximizing an approximate variational lower bound or equivalently by REINFORCE\n",
        "\n",
        "- main difference is the definition of the\n",
        "φ function \n",
        "\n",
        "- input: single raw image\n",
        "- output: caption y encoded as a sequence of 1-of-K encoded words.\n",
        "1. Encoder\n",
        "- convolutional neural network\n",
        "-  produces L vectors, each of which is\n",
        "a D-dimensional representation corresponding to a part of\n",
        "the image.\n",
        "- we used\n",
        "the Oxford VGGnet (Simonyan & Zisserman, 2014) pretrained on ImageNet without finetuning.\n",
        "- e 14×14×512 feature map of the fourth\n",
        "convolutional layer before max pooling. This means our\n",
        "decoder operates on the flattened 196 × 512 (i.e L × D)\n",
        "encoding.\n",
        "2. Decoder \n",
        "- LSTM\n",
        "3. Hard attention\n",
        "- \n",
        "4. Soft attention\n",
        "\n",
        "5. Training\n",
        "- Both variants of our attention model were trained with\n",
        "stochastic gradient descent using adaptive learning rate algorithms. \n",
        "- For the Flickr8k dataset, RMSProp\n",
        "- for\n",
        "Flickr30k/MS COCO dataset we used the recently proposed Adam algorithm\n"
      ],
      "metadata": {
        "id": "v_pVS-y-7Pdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
      ],
      "metadata": {
        "id": "gkbZ3uCJVWyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "iGueyt2-2CwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = VGG19(weights='imagenet')\n",
        "model = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output)\n",
        "\n",
        "img_path = 'elephant.jpg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "block4_pool_features = model.predict(x)"
      ],
      "metadata": {
        "id": "riIGwDDSKNx_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}